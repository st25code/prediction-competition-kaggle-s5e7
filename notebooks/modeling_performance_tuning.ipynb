{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31cd2f3c",
   "metadata": {},
   "source": [
    "# üöÄ Performance Optimization: Modeling Experiments\n",
    "\n",
    "This notebook explores different strategies to improve the performance of our models\n",
    "on the introvert vs extrovert classification task.\n",
    "\n",
    "While our baseline Logistic Regression already achieves high accuracy (~96.9%),\n",
    "we aim to squeeze out any additional gains using the following techniques:\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Goals\n",
    "\n",
    "- Reduce overfitting and noise\n",
    "- Improve generalization on hidden test data\n",
    "- Achieve accuracy closer to 0.9700+ or beyond\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Optimization Steps\n",
    "\n",
    "1. **Feature Selection**  \n",
    "   Use `SelectFromModel` and optionally `RFECV` to keep only meaningful features.\n",
    "\n",
    "2. **Model Stacking**  \n",
    "   Combine predictions from multiple base models using a meta-learner.\n",
    "\n",
    "3. **Hyperparameter Tuning**  \n",
    "   Use `GridSearchCV` to find optimal settings for Logistic Regression and XGBoost.\n",
    "\n",
    "4. **Optional: Sample Filtering**  \n",
    "   Remove rows with many missing values or conflicting patterns to reduce label noise.\n",
    "\n",
    "---\n",
    "\n",
    "We will evaluate each step using 5-fold cross-validation (`StratifiedKFold`)\n",
    "and compare the accuracy improvements step-by-step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e169bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 7 of 14 features\n",
      "Logistic Regression (selected features): 0.9678 ¬± 0.0021\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Feature Selection with SelectFromModel\n",
    "# We'll use Logistic Regression as the base estimator to identify and keep only the most impactful features.\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/train_processed.csv')\n",
    "X = df.drop(columns=['id', 'Personality'])\n",
    "y = LabelEncoder().fit_transform(df['Personality'])\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectFromModel(LogisticRegression(max_iter=1000, random_state=42), threshold='median')\n",
    "selector.fit(X, y)\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "print(f\"Selected {X_selected.shape[1]} of {X.shape[1]} features\")\n",
    "\n",
    "# Evaluate model with selected features\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "scores = cross_val_score(model, X_selected, y, cv=cv, scoring='accuracy')\n",
    "print(f\"Logistic Regression (selected features): {scores.mean():.4f} ¬± {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a5745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model: 0.9689 ¬± 0.0019\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Model Stacking\n",
    "# We will now combine Logistic Regression, Random Forest, and XGBoost using a meta-classifier (Logistic Regression).\n",
    "\n",
    "# %% Stacking model\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('xgb', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "]\n",
    "\n",
    "stacked = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=cv\n",
    ")\n",
    "\n",
    "# Evaluate stacked model\n",
    "stack_scores = cross_val_score(stacked, X, y, cv=cv, scoring='accuracy')\n",
    "print(f\"Stacked Model: {stack_scores.mean():.4f} ¬± {stack_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3211c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.9691\n",
      "Best parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# 3Ô∏è‚É£ Hyperparameter Tuning (Logistic Regression)\n",
    "# We'll use GridSearchCV to optimize the regularization strength `C` for Logistic Regression.\n",
    "\n",
    "# GridSearch for Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=2000, random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X, y)\n",
    "print(f\"Best CV accuracy: {grid_lr.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {grid_lr.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0e7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç XGBoost GridSearch Results:\n",
      "Best CV accuracy: 0.9691\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning (XGBoost)\n",
    "# We'll now tune key parameters for XGBoost to explore if better settings improve performance.\n",
    "\n",
    "# %% GridSearch for XGBoost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    estimator=XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    param_grid=xgb_params,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X, y)\n",
    "\n",
    "print(\"üîç XGBoost GridSearch Results:\")\n",
    "print(f\"Best CV accuracy: {grid_xgb.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {grid_xgb.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6043745",
   "metadata": {},
   "source": [
    "### üßæ Final Result Summary\n",
    "\n",
    "Despite multiple optimization attempts, no approach significantly outperformed the baseline model.\n",
    "\n",
    "| Approach                     | Accuracy ¬± Std      |\n",
    "|-----------------------------|---------------------|\n",
    "| Baseline Logistic Regression | 0.9690 ¬± 0.0017     |\n",
    "| Feature Selection (SFM)     | 0.9678 ¬± 0.0021     |\n",
    "| Stacking                    | 0.9689 ¬± 0.0019     |\n",
    "| Tuned Logistic Regression   | 0.9691 ¬± 0.0017     |\n",
    "| Tuned XGBoost               | 0.9691 ¬± 0.0017     |\n",
    "\n",
    "**Conclusion:** The baseline model already performs near-optimally.  \n",
    "Further improvements may require smarter feature engineering and better noise handling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
